# Dockerfile for DeepSeek-OCR Server with vLLM
# Optimized for RunPod deployment
# Based on: https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html

FROM nvidia/cuda:12.1.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH="${CUDA_HOME}/bin:${PATH}"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    python3-dev \
    git \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN pip3 install --upgrade pip setuptools wheel

# Install Python dependencies
RUN pip3 install --no-cache-dir \
    flask>=3.0.0 \
    pillow>=10.0.0 \
    gunicorn>=21.0.0

# Install vLLM from nightly (until v0.11.1 release)
# This is the key dependency for optimized DeepSeek-OCR inference
RUN pip3 install --no-cache-dir -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly

# Set working directory
WORKDIR /app

# Copy server code
COPY deepseek_ocr_server.py /app/

# Expose port
EXPOSE 5003

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:5003/health || exit 1

# Run with gunicorn for production (or directly with python for development)
# For RunPod, we'll use direct python since it's a single GPU instance
CMD ["python3", "deepseek_ocr_server.py"]

# Alternative with gunicorn for production:
# CMD ["gunicorn", "--bind", "0.0.0.0:5003", "--workers", "1", "--timeout", "300", "--worker-class", "sync", "deepseek_ocr_server:app"]


